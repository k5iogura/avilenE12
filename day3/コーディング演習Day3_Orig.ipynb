{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"コーディング演習Day3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MY_aiqxYOG6T"},"source":["# 【全人類がわかるE資格コース】コーディング演習Day3\n","\n","## 概要\n","\n","本演習では深層学習の基礎である多層パーセプトロンによる学習を穴埋め形式で実装します。なお、予め用意されたコードはそのまま使用し、指示された穴埋め部を編集してください。\n","演習問題文は<font color=\"Red\">赤字</font>です。このファイルは必ず最後までコードをすべて実行し、「最後までコードが実行可能」・「学習結果の出力がある」・「学習が成功している」の３つを満たした状態で提出してください。\n","\n","また、乱数設定により実行結果が異なるため、<font color=\"Red\">コードを完成させたあと、必ずもう一度一番上のセルから順に最後まで実行して結果を確認してください。</font>\n","\n","所要時間：4~8時間"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7FW9tbQuOG6U"},"source":["## ライブラリのインポート\n","\n","必要なライブラリをインポートします。エラーになる場合は該当するものをインストールしてください。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Jr7dWekfOG6V","colab":{}},"source":["import os\n","import pickle\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","%matplotlib inline\n","# 乱数シードを指定\n","np.random.seed(seed=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hEcnF2whOG6X","colab":{}},"source":["if os.path.exists('mnist_784'):\n","    with open('mnist_784','rb') as f:\n","        mnist = pickle.load(f)\n","else:\n","    mnist = datasets.fetch_openml('mnist_784')\n","    with open('mnist_784', 'wb') as f:\n","        pickle.dump(mnist, f)\n","# 画像とラベルを取得\n","X, T = mnist.data, mnist.target\n","# 訓練データとテストデータに分割\n","x_train, x_test, t_train, t_test = train_test_split(X, T, test_size=0.2)\n","\n","# ラベルデータをint型にし、one-hot-vectorに変換します\n","t_train = np.eye(10)[t_train.astype(\"int\")]\n","t_test = np.eye(10)[t_test.astype(\"int\")]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Emh7bUhBOG6Z"},"source":["## データの説明\n","\n","mnist と呼ばれる手書き数字の認識問題である。\n","\n","データは 784 次元の配列となっています。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oFpveFljOG6Z","scrolled":true,"colab":{}},"source":["# データを5つ表示\n","for i in range(5):\n","    plt.gray()\n","    plt.imshow(x_train[i].reshape((28,28)))\n","    plt.show()\n","    print(\"label: \", t_train[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Bouk_Et-OG6b"},"source":["## Optimizer の実装"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EPWM5OYiOG6c"},"source":["### 確率的勾配降下法\n","\n","1-1. <font color=\"Red\">確率的勾配降下法を用いたOptimizerのクラス SGD を完成させてください。</font>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SNsQV6acOG6d","colab":{}},"source":["class SGD:\n","\n","    def __init__(self, lr=0.01):\n","        self.lr = lr\n","        \n","    def update(self, params, grads):\n","        for key in params.keys():\n","            ###### 問1-1 ######"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n8S61WJZOG6e"},"source":["### Adam\n","\n","1-2. <font color=\"Red\">Adamを用いたOptimizerのクラス Adam を完成させてください。</font><br>\n","Adamの特徴としてハイパーパラメータのバイアス補正(偏りの補正)が行われることが挙げられます。書籍『ゼロから作るDeepLearning』の配布コードでは簡易版のため、バイアス補正を組み込んでいません。この問題ではバイアス補正を組み込んだ完成形のAdamコードについて回答してください。また、過去のE資格試験ではこちらの完成形のAdamコードが出題されています。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sdM19jOEOG6f","colab":{}},"source":["class Adam:\n","\n","    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n","        self.lr = lr\n","        self.beta1 = beta1\n","        self.beta2 = beta2\n","        self.iter = 0\n","        self.m = None\n","        self.v = None\n","        \n","    def update(self, params, grads):\n","        if self.m is None:\n","            self.m, self.v = {}, {}\n","            for key, val in params.items():\n","                self.m[key] = np.zeros_like(val)\n","                self.v[key] = np.zeros_like(val)\n","        \n","        self.iter += 1\n","        \n","        for key in params.keys():\n","            self.m[key] = ###### 問1-2-1 ######\n","            self.v[key] = ###### 問1-2-2 ######\n","            m_unbias = ###### 問1-2-3 ######\n","            v_unbias = ###### 問1-2-4 ######\n","            params[key] -= self.lr * m_unbias / (np.sqrt(v_unbias) + 1e-7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R-NaWscJOG6g"},"source":["## コスト関数\n","\n","多クラス分類問題なので、クロスエントロピーをコスト関数して用います。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kwiPxmeMOG6h","colab":{}},"source":["def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OL6uU4NXOG6i","colab":{}},"source":["def softmax(x):\n","    x = x.T\n","    _x = x - np.max(x, axis=0)\n","    _x = np.exp(_x) / np.sum(np.exp(_x), axis=0)\n","    return _x.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y78Ii3foOG6k"},"source":["## ネットワークの実装\n","\n","まずはバッチ正規化を入れない普通の三層ニューラルネットワークを実装します。問題にはなっていませんが、day1の復習も兼ねてコードを読み理解しておいてください。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YkgxEeUhOG6l","colab":{}},"source":["class mnistMultiLayerNet:\n","    \"\"\"\n","    layer0: 784 次元の入力\n","    ↓ w1, b1 で線形結合\n","    ↓ relu で活性化\n","    layer1: 100 次元の隠れ層\n","    ↓ w2, b2 で線形結合\n","    ↓ relu で活性化\n","    layer2: 100 次元の隠れ層\n","    ↓ w3, b3 で線形結合\n","    ↓ relu で活性化\n","    layer3: 100 次元の隠れ層\n","    ↓ w4, b4 で線形結合\n","    ↓ relu で活性化\n","    layer4: 100 次元の隠れ層\n","    ↓ w5, b5 で線形結合\n","    layer5: 10 次元の出力層\n","    \"\"\"\n","    def __init__(self):\n","        self.input_size = 784\n","        self.output_size = 10\n","        self.hidden_size_list = [100, 100, 100, 100]\n","        self.all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n","        self.hidden_layer_num = len(self.hidden_size_list)\n","        self.weight_decay_lambda =0\n","        self.params = {}\n","        self.layers = {}\n","        self.grads = {}\n","\n","        # 重みとバイアスの初期化\n","        for idx in range(1, len(self.all_size_list)):\n","            self.params['w' + str(idx)] = np.random.randn(self.all_size_list[idx-1], self.all_size_list[idx]) * 0.085\n","            self.params['b' + str(idx)] = np.zeros(self.all_size_list[idx], dtype=float)\n","\n","        \n","    def forward(self, x):\n","        relu = lambda x : np.maximum(0, x)  # 活性化関数として ReLU を使用\n","        self.layers['layer0'] = x\n","        for idx in range(1, len(self.all_size_list) - 1):\n","            w = self.params['w' + str(idx)]\n","            b = self.params['b' + str(idx)]\n","            x = self.layers['layer' + str(idx - 1)]\n","            self.layers['layer' + str(idx)] = relu(np.dot(x, w) + b)\n","        idx = len(self.all_size_list) - 1\n","        w = self.params['w' + str(idx)]\n","        b = self.params['b' + str(idx)]\n","        x = self.layers['layer' + str(idx - 1)]\n","        self.layers['layer' + str(idx)] = softmax(np.dot(x, w) + b)\n","        \n","        return self.layers['layer' + str(idx)]\n","        \n","\n","    def loss(self, y, t):\n","        return cross_entropy_error(y, t)\n","    \n","    def backward(self, t, y):\n","        delta = (y - t) / t.shape[0]\n","        self.grads['b5'] = np.sum(delta, axis=0)\n","        self.grads['w5'] = np.dot(self.layers['layer4'].transpose(), delta)\n","        # 誤差逆伝播\n","        for idx in range(4, 0, -1):\n","            delta = np.dot(delta, self.params['w' + str(idx + 1)].transpose())\n","            delta = delta *  (self.layers['layer' + str(idx)] > 0)\n","            self.grads['b' + str(idx)] = np.sum(delta, axis=0)\n","            self.grads['w' + str(idx)] = np.dot(self.layers['layer'+str(idx - 1)].transpose(), delta)\n","        return self.grads\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZGnDfiCwOG6m"},"source":["## バッチ正規化を用いるネットワーク\n","\n","各層について、重みを掛けて足し合わせた後バッチ正規化を行う。\n","\n","2. <font color=\"Red\">バッチ正規化を用いたニューラルネットワークを完成させてください。</font>\n","\n","  バッチ正規化の順伝播は以下の式に従って実装します。\n","  \n","  - （訓練時のみ）まずは計算しているミニバッチについて、平均と分散を求めます。各次元について、全データを通じた平均・分散を計算するため、平均・分散を計算する軸にご注意ください。\n","\n","  - （訓練時のみ）テスト時に使用するために、訓練データ全体での平均を推定します。モーメンタム $m$ を用いて今までの平均 $\\mu_{old} $ を計算しているミニバッチの平均 $\\mu$ の方向に移動させ、新しい平均$\\mu_{new} $を求めます。\n","  $$\n","  \\mu_{new} = m \\mu_{old} + ( 1 - m)\\mu\\tag{1}\n","  $$\n","\n","  - （訓練時のみ）同様に今までの分散 $\\sigma_{old} ^ 2$ を計算しているミニバッチの平均 $\\sigma^2$の方向に移動させ、 新しい分散$\\sigma_{new}^2$ を求めます。\n","  $$\n","  \\sigma_{new}^2 = m \\sigma_{old}^2 + ( 1 - m)\\sigma^2\\tag{2}\n","  $$\n","\n","  - 求めた平均 $\\mu$ と分散 $\\sigma^2$ を用いて、入力 $x$ を正規化した値 $x_n$ を求めます。分散$\\sigma^2$から標準偏差 $\\sigma$ を求めるときに、アンダーフローを避けるために 10e-7 ($10 \\times 10 ^ {-7}$) を足してから平方根を取っています。\n","  テスト時には、移動平均により推定した訓練データ全体での平均・分散を使用します。\n","  $$\n","  \\sigma = \\sqrt{\\sigma ^ 2 + 10 \\times 10 ^ {-7}}\\tag{3}\n","  $$\n","  $$\n","  x_n = (x - \\mu) / \\sigma\\tag{4}\n","  $$\n","\n","   - 正規化した値 $x_n$に対して $\\gamma$ を用いて変倍し、$\\beta$ を用いて移動を行い、活性化関数に渡す出力 $y$ を求めます。\n","   $$\n","   y = \\gamma x_n + \\beta\\tag{5}\n","   $$\n"," \n","   バッチ正規化の誤差逆伝播は以下の式に従って実装します。\n","   \n","   - 直前まで逆伝播してきた$1, 2, \\dots , N$ 番目(Nはバッチサイズ)の出力データ$y_k$による勾配 $\\frac{\\partial L}{\\partial y_k}$を用いて $\\gamma$ と$\\beta$による勾配を計算します。 $x_{nk}$ はミニバッチの中のk番目の入力データを正規化した後の値を表します。\n","   \n","   $$\n","   \\begin{eqnarray} \n","   \\frac{\\partial L}{\\partial \\gamma} & = & \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial \\gamma} =\\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y_k} x_{nk}\\tag{6}\\\\\n","   \\frac{\\partial L}{\\partial \\beta} & = & \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial \\beta} =            \\sum_{k=1}^{N} \\frac{\\partial L}{\\partial y_k}\\tag{7}\n","   \\end{eqnarray}\n","   $$\n","   \n","   - $1, 2, \\dots , N$ 番目の入力データ$x_k$による勾配 $\\frac{\\partial L}{\\partial x_k}$を計算します（コードでは高速化のため少々異なった計算をしています）。\n","   \n","   $$\n","  \\begin{equation} \n","  \\frac{\\partial L}{\\partial x_k} \n","  = \\frac{\\gamma}{\\sigma} \\Bigg[ \\frac{\\partial L}{\\partial y_k} \n","  - \\frac{1}{N} \\bigg[ \\frac{\\partial L}{\\partial \\beta} + x_{nk} \\frac{\\partial L}{\\partial \\gamma} \\bigg] \\Bigg] \n","  \\end{equation}\n","  $$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zavAJFafOG6n","colab":{}},"source":["class mnistMultiLayerBatchNet:\n","    \"\"\"\n","    layer0: 784 次元の入力\n","    ↓ w1, b1 で線形結合\n","    ↓バッチ正規化 gamma1倍しbeta1だけずらす\n","    ↓ relu で活性化\n","    layer1: 100 次元の隠れ層\n","    ↓ w2, b2 で線形結合\n","    ↓バッチ正規化 gamma2倍しbeta2だけずらす\n","    ↓ relu で活性化\n","    layer2: 100 次元の隠れ層\n","    ↓ w3, b3 で線形結合\n","    ↓バッチ正規化 gamma3倍しbeta3だけずらす\n","    ↓ relu で活性化\n","    layer3: 100 次元の隠れ層\n","    ↓ w4, b4 で線形結合\n","    ↓バッチ正規化 gamma4倍しbeta4だけずらす\n","    ↓ relu で活性化\n","    layer4: 100 次元の隠れ層\n","    ↓ w5, b5 で線形結合\n","    layer5: 10 次元の出力層\n","    \"\"\"\n","    def __init__(self):\n","        self.input_size = 784\n","        self.output_size = 10\n","        self.hidden_size_list = [100, 100, 100, 100]\n","        self.all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n","        self.hidden_layer_num = len(self.hidden_size_list)\n","        self.weight_decay_lambda =0\n","        self.params = {}\n","        self.layers = {}\n","        self.grads = {}\n","        self.norms = {}\n","        self.momentum = 0.9\n","\n","        # パラメータの初期化\n","        for idx in range(1, len(self.all_size_list)):\n","            # 線形結合層のパラメータ\n","            self.params['w' + str(idx)] = np.random.randn(self.all_size_list[idx-1], self.all_size_list[idx]) * 0.085\n","            self.params['b' + str(idx)] = np.zeros(self.all_size_list[idx], dtype=float)\n","            \n","            # バッチ正規化でシフトさせるときに用いるγとβを更新するパラメータとし初期化\n","            # mu と sigma は実行時の平均と分散\n","            if idx != len(self.all_size_list) - 1:\n","                self.params['gamma' + str(idx)] = np.ones(self.all_size_list[idx])\n","                self.params['beta' + str(idx)] = np.zeros(self.all_size_list[idx])\n","                self.norms['mu' + str(idx)] = None\n","                self.norms['var' + str(idx)] = None\n","        \n","    def forward(self, x, train_flg=False):\n","        relu = lambda x : np.maximum(0, x)  # 活性化関数として ReLU を使用\n","        self.layers['layer0'] = x\n","        for idx in range(1, len(self.all_size_list) - 1):\n","            # 線形結合層\n","            w = self.params['w' + str(idx)]\n","            b = self.params['b' + str(idx)]\n","            x = self.layers['layer' + str(idx - 1)]\n","            x = np.dot(x, w) + b\n","            \n","            # バッチ正規化\n","            # 平均と分散の初期化\n","            if self.norms['mu' + str(idx)] is None:\n","                N, D = x.shape\n","                self.norms['mu' + str(idx)] = np.zeros(D)\n","                self.norms['var' + str(idx)] = np.zeros(D)\n","            if train_flg:\n","                mu = ###### 問2.1 ######          # 今回のミニバッチの平均\n","                xc = x - mu                   # 今回のミニバッチの平均との差分\n","                var = ###### 問2.2 ######  # 今回のミニバッチの分散\n","                std = np.sqrt(var + 10e-7)    # 今回のミニバッチの標準偏差\n","                xn = xc / std                 # 正規化\n","\n","                # 全体の平均と分散を移動平均により求める(1),(2)\n","                self.norms['mu' + str(idx)] = ###### 問2.3 ######\n","                self.norms['var' + str(idx)] = ###### 問2.4 ######\n","                \n","                # 誤差逆伝播で使う中間データ\n","                self.norms['xc' + str(idx)] = xc\n","                self.norms['xn' + str(idx)] = xn\n","                self.norms['std' + str(idx)] = std\n","                self.norms['size' + str(idx)] = x.shape[0]\n","            else:\n","                # テスト時は全体の平均と分散を使って正規化する(3),(4)\n","                xc = ###### 問2.5 ######\n","                xn = ###### 問2.6 ######\n","                \n","            # バッチ正規化でシフトさせる(5)\n","            shifted = ###### 問2.7 ######\n","            \n","            # relu を使って活性化\n","            self.layers['layer' + str(idx)] = relu(shifted)\n","\n","        # 出力層\n","        idx = len(self.all_size_list) - 1\n","        w = self.params['w' + str(idx)]\n","        b = self.params['b' + str(idx)]\n","        x = self.layers['layer' + str(idx - 1)]\n","        self.layers['layer' + str(idx)] = softmax(np.dot(x, w) + b)\n","        \n","        return self.layers['layer' + str(idx)]\n","        \n","\n","    def loss(self, y, t):\n","        return cross_entropy_error(y, t)\n","    \n","    def backward(self, t, y):\n","        # 出力層における誤差の勾配（クロスエントロピー関数の勾配）\n","        delta = (y - t) / t.shape[0]\n","        \n","        # 出力層手前の線形結合層における勾配の逆伝播\n","        self.grads['b5'] = np.sum(delta, axis=0)\n","        self.grads['w5'] = np.dot(self.layers['layer4'].transpose(), delta)\n","        \n","        # 誤差逆伝播\n","        for idx in range(4, 0, -1):\n","            delta = np.dot(delta, self.params['w' + str(idx + 1)].transpose())\n","            \n","            # relu の微分\n","            delta = delta *  (self.layers['layer' + str(idx)] > 0)\n","            \n","            # バッチ正規化における勾配の逆伝播(6),(7)\n","            self.grads['beta' + str(idx)] = ###### 問2.8 ######\n","            self.grads['gamma' + str(idx)] = ###### 問2.9 ######\n","            dxn = self.params['gamma' + str(idx)] * delta\n","            dxc = dxn / self.norms['std' + str(idx)]\n","            dstd = -np.sum((dxn * self.norms['xc' + str(idx)]) / (self.norms['std' + str(idx)] * self.norms['std' + str(idx)]), axis=0)\n","            dvar = 0.5 * dstd / self.norms['std' + str(idx)]\n","            dxc += (2.0 / self.norms['size' + str(idx)]) * self.norms['xc' + str(idx)] * dvar\n","            dmu = np.sum(dxc, axis=0)\n","            delta = dxc - dmu / self.norms['size' + str(idx)]\n","            \n","            # 線形結合層における勾配の逆伝播\n","            self.grads['b' + str(idx)] = np.sum(delta, axis=0)\n","            self.grads['w' + str(idx)] = np.dot(self.layers['layer'+str(idx - 1)].transpose(), delta)\n","            \n","        return self.grads\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2QrrsrJHOG6p"},"source":["## ミニバッチを用いた学習\n","\n","3. <font color=\"Red\">ミニバッチサイズ128に分割して学習させるように以下のプログラムを完成させてください。</font>\n"," - xとtの対応を保ったままシャッフルさせたのち、バッチサイズ分だけデータを取り出します。\n"," - ヒント: numpy.random.permutation を用いることで、データのインデックスをシャッフルした配列を用意することで、シャッフルインデックス配列permに対して、前からバッチサイズずつインデックスを切り出せばミニバッチの抽出が行えます。\n"," - また、学習用のコードは実行に時間がかかります。完了するまで5~10分ほどを要しますのでご注意ください。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SR705KyHOG6p","colab":{}},"source":["bn = mnistMultiLayerBatchNet()\n","nobn = mnistMultiLayerNet()\n","adambn = mnistMultiLayerBatchNet()\n","adamnobn = mnistMultiLayerNet()\n","\n","bn_acc_list = []\n","nobn_acc_list = []\n","adambn_acc_list = []\n","adamnobn_acc_list = []\n","\n","sgd = SGD(lr = 0.01)\n","adam = Adam(lr=0.01)\n","\n","# ミニバッチアルゴリズム\n","batch_size = 128"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hL36vPejOG6r","scrolled":true,"colab":{}},"source":["for epoch in range(20):\n","    # ランダムにミニバッチへ分割するために、インデックスをランダムに並び替える\n","    perm = ###### 問3.1 ######\n","    \n","    # batch_size ごとにデータを読み込んで学習させる\n","    for idx in ###### 問3.2 ######:\n","        x = ###### 問3.3 ######\n","        t =  ###### 問3.4 ######\n","        \n","        y = bn.forward(x, train_flg=True)\n","        grads = bn.backward(t, y)\n","        sgd.update(bn.params,grads)\n","        \n","        y = adambn.forward(x, train_flg=True)\n","        grads = adambn.backward(t, y)\n","        adam.update(adambn.params,grads)\n","        \n","        y = nobn.forward(x)\n","        grads = nobn.backward(t,y)\n","        sgd.update(nobn.params, grads)\n","        \n","        y = adamnobn.forward(x)\n","        grads = adamnobn.backward(t, y)\n","        adam.update(adamnobn.params,grads)\n","\n","    y_test = bn.forward(x_test)\n","    bn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n","    y_test = nobn.forward(x_test)\n","    nobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n","    y_test = adambn.forward(x_test)\n","    adambn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n","    y_test = adamnobn.forward(x_test)\n","    adamnobn_acc_list.append((y_test.argmax(axis=1) == t_test.argmax(axis=1)).mean())\n","\n","    print(f'EPOCH {epoch + 1} | NoBatch ACCURACY (SGD) {nobn_acc_list[-1]:.2%} | Batch ACCURACY (SGD){bn_acc_list[-1]:.2%} | NoBatch ACCURACY (Adam){adamnobn_acc_list[-1]:.2%} | Batch ACCURACY (Adam) {adambn_acc_list[-1]:.2%}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JYmN3xRhOG6t"},"source":["## 学習結果\n","学習結果を可視化してみます。まずはSGDを使った学習についてのみ比較を行います。結果のグラフが以下のグラフと一致していれば学習は成功しています。\n","<img src=\"sgd.png\">\n","学習結果からわかる通り、バッチ正規化を加えることでテスト精度が高い水準で安定させることが可能となります。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fBfEm6vvOG6t","scrolled":false,"colab":{}},"source":["x = range(20)\n","plt.plot(x, bn_acc_list, color='turquoise')\n","plt.plot(x, nobn_acc_list, color='tomato')\n","\n","plt.legend(['BatchNormalization', 'Normal Network'])\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bKh5jSYNOG6v"},"source":["次にAdamで学習した場合も含めてプロットしてみましょう。結果のグラフの特徴が以下と一致していれば学習成功です。<br>\n","・Adamで学習すると、SGDよりも学習が進みやすく、高い精度が実現できている。<br>\n","・バッチ正規化を加えないネットワークでは過学習により途中からテスト精度が急に低下している。<br>\n","バッチ正規化を加えることで正則化の役割も可能となっていることが読み取れます。\n","<img src=\"adam.png\">"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MWW5j2lWOG6w","colab":{}},"source":["x = range(20)\n","plt.plot(x, bn_acc_list, color='turquoise', linestyle = '-')\n","plt.plot(x, nobn_acc_list, color='tomato', linestyle='-')\n","plt.plot(x, adambn_acc_list, color='turquoise', linestyle = '--')\n","plt.plot(x, adamnobn_acc_list, color='tomato', linestyle='--')\n","\n","plt.ylim((0.85, 1))\n","\n","plt.legend(['BatchNormalization(SGD)', 'Normal Network(SGD)', 'BatchNormalization(Adam)', 'Normal Network(Adam)'])\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.show()"],"execution_count":null,"outputs":[]}]}